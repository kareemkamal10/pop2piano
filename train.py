"""
Pop2Piano Training Script - Complete Implementation

Features:
- Mixed Precision (FP16) for faster training
- Gradient Checkpointing for memory efficiency  
- Resume from checkpoint
- TensorBoard logging
- Early stopping
- Multi-GPU support
"""

import os
import sys
import argparse
from datetime import datetime

import torch
try:
    import lightning as pl
    from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
    from lightning.pytorch.loggers import TensorBoardLogger
except ImportError:
    import pytorch_lightning as pl
    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
    from pytorch_lightning.loggers import TensorBoardLogger

from omegaconf import OmegaConf

# Import project modules
from transformer_wrapper import TransformerWrapper
from midi_tokenizer import MidiTokenizer
from dataset import Pop2PianoDataset, Pop2PianoCollator, create_dataloaders


def parse_args():
    parser = argparse.ArgumentParser(description='Train Pop2Piano Model')
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file')
    parser.add_argument('--data_dir', type=str, default='output_dir', help='Directory with preprocessed data')
    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint to resume from')
    parser.add_argument('--batch_size', type=int, default=None, help='Override batch size')
    parser.add_argument('--epochs', type=int, default=None, help='Override max epochs')
    parser.add_argument('--lr', type=float, default=None, help='Override learning rate')
    parser.add_argument('--precision', type=str, default='16-mixed', choices=['32', '16-mixed', 'bf16-mixed'], 
                        help='Training precision')
    parser.add_argument('--debug', action='store_true', help='Debug mode with small dataset')
    return parser.parse_args()


def main():
    args = parse_args()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Load Configuration
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"Config file not found: {args.config}")
    
    config = OmegaConf.load(args.config)
    print(f"âœ… Configuration loaded from {args.config}")
    
    # Apply command line overrides
    if args.batch_size:
        config.training.batch_size = args.batch_size
    if args.epochs:
        config.training.max_epochs = args.epochs
    if args.lr:
        config.training.lr = args.lr
    
    # Set seed for reproducibility
    pl.seed_everything(config.training.seed, workers=True)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. Setup Data
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    data_dir = args.data_dir
    
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f"âš ï¸  '{data_dir}' was missing. Created it.")
        print("ğŸ’¡ Run download.py and preprocess scripts first!")
        return
    
    # Create tokenizer
    tokenizer = MidiTokenizer(config.tokenizer)
    
    # Create dataloaders
    print(f"\nğŸ“¦ Loading data from: {data_dir}")
    
    # Adjust workers for Windows
    num_workers = 0 if sys.platform == 'win32' else config.training.num_workers
    
    train_loader, val_loader = create_dataloaders(
        data_dir=data_dir,
        config=config,
        tokenizer=tokenizer,
        batch_size=config.training.batch_size,
        num_workers=num_workers,
    )
    
    if len(train_loader.dataset) == 0:
        print("âŒ No training data found!")
        print("ğŸ’¡ Make sure to download and preprocess data first.")
        return
    
    print(f"ğŸ“Š Training samples: {len(train_loader.dataset)}")
    print(f"ğŸ“Š Validation samples: {len(val_loader.dataset)}")
    print(f"ğŸ“Š Batch size: {config.training.batch_size}")
    print(f"ğŸ“Š Training batches: {len(train_loader)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. Setup Model
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ§  Initializing Model...")
    
    if args.resume and os.path.exists(args.resume):
        print(f"ğŸ“‚ Resuming from checkpoint: {args.resume}")
        model = TransformerWrapper.load_from_checkpoint(args.resume, config=config)
    else:
        model = TransformerWrapper(config)
    
    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ“Š Trainable parameters: {trainable_params:,}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. Setup Callbacks
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    callbacks = [
        # Save best models
        ModelCheckpoint(
            dirpath=f"checkpoints/{timestamp}",
            filename="pop2piano-{epoch:02d}-{val_loss:.4f}",
            save_top_k=3,
            monitor="val_loss",
            mode="min",
            save_last=True,  # Always save last checkpoint for resume
        ),
        
        # Early stopping
        EarlyStopping(
            monitor="val_loss",
            patience=10,
            mode="min",
            verbose=True,
        ),
        
        # Learning rate monitor
        LearningRateMonitor(logging_interval='step'),
    ]
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. Setup Logger
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger = TensorBoardLogger(
        save_dir="logs",
        name="pop2piano",
        version=timestamp,
    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. Setup Trainer
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâš™ï¸  Setting up Trainer...")
    
    # Determine accelerator and devices
    if torch.cuda.is_available():
        accelerator = "cuda"
        devices = min(config.training.num_gpu, torch.cuda.device_count())
        print(f"ğŸ® Using {devices} GPU(s)")
    else:
        accelerator = "cpu"
        devices = 1
        print("ğŸ’» Using CPU (training will be slow)")
    
    trainer = pl.Trainer(
        max_epochs=config.training.max_epochs,
        accelerator=accelerator,
        devices=devices,
        precision=args.precision,  # Mixed precision for speed
        callbacks=callbacks,
        logger=logger,
        
        # Gradient settings
        gradient_clip_val=config.training.gradient_clip_val,
        accumulate_grad_batches=config.training.accumulate_grad_batches,
        
        # Validation
        check_val_every_n_epoch=config.training.check_val_every_n_epoch,
        
        # Performance
        enable_progress_bar=True,
        enable_model_summary=True,
        
        # Debug mode
        fast_dev_run=args.debug,
        
        # Deterministic for reproducibility
        deterministic=True,
    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 7. Start Training
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸš€ Starting Training...")
    print(f"ğŸ“Š Precision: {args.precision}")
    print(f"ğŸ“Š Max epochs: {config.training.max_epochs}")
    print(f"ğŸ“Š Learning rate: {config.training.lr}")
    print(f"ğŸ“Š Optimizer: {config.training.optimizer}")
    print("-" * 50)
    
    try:
        trainer.fit(
            model, 
            train_dataloaders=train_loader,
            val_dataloaders=val_loader,
            ckpt_path=args.resume if args.resume and os.path.exists(args.resume) else None,
        )
        
        print("\nâœ… Training completed!")
        print(f"ğŸ“ Checkpoints saved to: checkpoints/{timestamp}")
        print(f"ğŸ“ Logs saved to: logs/pop2piano/{timestamp}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user")
        print("ğŸ’¡ You can resume training using --resume <checkpoint_path>")
        
    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        raise


if __name__ == "__main__":
    main()
